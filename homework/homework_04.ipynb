{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d21e1ead",
      "metadata": {
        "id": "d21e1ead"
      },
      "source": [
        "# HW04: ML and DL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680d1f0b",
      "metadata": {
        "id": "680d1f0b"
      },
      "source": [
        "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bf38c8",
      "metadata": {
        "id": "c9bf38c8"
      },
      "source": [
        "## Load and Pre-process Text\n",
        "We do sentiment analysis on the [Movie Review Data](https://www.cs.cornell.edu/people/pabo/movie-review-data/). If you would like to know more about the data, have a look at [the paper](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf) (but no need to do so)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21439804",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21439804",
        "outputId": "a30baf4d-a3c9-4ccb-b667-7991ae9a5c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-12 04:34:27--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4029756 (3.8M) [application/x-gzip]\n",
            "Saving to: ‘scale_data.tar.gz’\n",
            "\n",
            "scale_data.tar.gz   100%[===================>]   3.84M  19.0MB/s    in 0.2s    \n",
            "\n",
            "2023-03-12 04:34:28 (19.0 MB/s) - ‘scale_data.tar.gz’ saved [4029756/4029756]\n",
            "\n",
            "--2023-03-12 04:34:28--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8853204 (8.4M) [application/x-gzip]\n",
            "Saving to: ‘scale_whole_review.tar.gz’\n",
            "\n",
            "scale_whole_review. 100%[===================>]   8.44M  11.7MB/s    in 0.7s    \n",
            "\n",
            "2023-03-12 04:34:29 (11.7 MB/s) - ‘scale_whole_review.tar.gz’ saved [8853204/8853204]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# In this tutorial, we do sentiment analysis\n",
        "# download the data\n",
        "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "#!tar xf aclImdb_v1.tar.gz\n",
        "\n",
        "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
        "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
        " \n",
        "!tar xf scale_data.tar.gz \n",
        "!tar xf scale_whole_review.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d685ef2e",
      "metadata": {
        "id": "d685ef2e"
      },
      "source": [
        "First, we have to load the data for which we provide the function below. Note how we also preprocess the text using gensim's simple_preprocess() function and how we already split the data into a train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18a238d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a18a238d",
        "outputId": "39b85e2c-668f-47d4-f332-6edfe679a0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: stanley ipkiss whose letter to the local paper signed nice guys finish last had generated torrent of replies the year before has been undergoing change lately bank clerk ipkiss played with sweet sincerity by jim carrey discovers mask that like dr jekyll potion temporarily creates an all new person to understand how the mask works he turns to masks that people wear expert named dr neuman played with dripping sincerity and dead pan humor by ben stein although the doctor proves useless stanley finally discovers for himself what the mask does it magnifies your inner desires since ipkiss is an incurable romantic who spends his free time watching cartoons it is inevitable that the mask turns him into the world greatest lover and song and dance man after avoiding carrey for years was blown away by his performance in liar liar one of this year funniest films since the mask in was the movie that really launched his film career suggested we check it out one evening on vacation with the help of realistic and colorful special effects carrey as the mask struts his stuff non stop when he meets his heart throb cameron diaz in her film debut as blond bombshell tina carlyle at nightclub his heart jumps out of his body and his jaw drops open far enough for yard long tongue to drool out carrey shows off his ability to impersonate countless other actors and reenact their most famous scenes when trapped by bad guys with machine guns he pulls out two cartoonish cannon like guns with dozen barrels each you have to ask yourself question he warns with soft clint eastwood voice do feel lucky ha nguyen stream of elaborate costumes for the mask sets the tone for all of the mask emotions when the mask is trapped by an army of police he switches to latin costume and soon has everyone formed into singing and dancing conga line in highly imaginative film the only surprise is how slowly director chuck russell paces the non mask scenes although it never got the belly laughs out of me that liar liar did the mask delivers some well choreographed numbers and displays carrey talents well still must confess that my favorite character in the film was not stanley but milo his little pooch why cute animals like max who plays milo do not get more acting roles in the movies remains mystery the mask runs it is rated pg for some cartoonish violence and some profanity most of it is so mild that the film should be fine for kids around and up my son jeffrey age thought the show was really good and funny recommend the picture to you and give it must see film excellent show look for it average movie kind of enjoyable poor show don waste your money totally and painfully unbearable picture review written on july opinions expressed are mine and not meant to reflect my employer \n",
            "label: 0.7\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from gensim.utils import simple_preprocess\n",
        "def load_data():\n",
        "    examples, labels = [], []\n",
        "    authors = os.listdir(\"scale_whole_review\")\n",
        "    for author in authors:\n",
        "        path = os.listdir(os.path.join(\"scale_whole_review\", author, \"txt.parag\"))\n",
        "        fn_ids = os.path.join(\"scaledata\", author, \"id.\" + author)\n",
        "        fn_ratings = os.path.join(\"scaledata\", author, \"rating.\" + author)\n",
        "        with open(fn_ids) as ids, open(fn_ratings) as ratings:\n",
        "            for idx, rating in zip(ids, ratings):\n",
        "                labels.append(float(rating.strip()))\n",
        "                filename_text = os.path.join(\"scale_whole_review\", author, \"txt.parag\", idx.strip() + \".txt\")\n",
        "                with open(filename_text, encoding='latin-1') as f:\n",
        "                    examples.append(\" \".join(simple_preprocess(f.read())))\n",
        "    return examples, labels\n",
        "                  \n",
        "X,y  = load_data()\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "print (\"text:\", X_train[0], \"\\nlabel:\", y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284033cf",
      "metadata": {
        "id": "284033cf"
      },
      "source": [
        "## Vectorize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09aff185",
      "metadata": {
        "id": "09aff185"
      },
      "outputs": [],
      "source": [
        "# train a TF_IDF Vectorizer on X_train and vectorize X_train and X_test\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer(min_df=0.01, # at min 1% of docs\n",
        "                        max_df=.5,  \n",
        "                        stop_words='english',\n",
        "                        ngram_range=(1,2))\n",
        "\n",
        "##TODO train vectorizer\n",
        "\n",
        "##TODO transform X_train to TF-IDF values\n",
        "X_train_tfidf = ...\n",
        "##TODO transform X_test to TF-IDF values\n",
        "X_test_tfidf = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d44dbb",
      "metadata": {
        "id": "58d44dbb"
      },
      "outputs": [],
      "source": [
        "##TODO scale both training and test data with the standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(with_mean=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9d8a57",
      "metadata": {
        "id": "ad9d8a57"
      },
      "source": [
        "## ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5f4520",
      "metadata": {
        "id": "1e5f4520"
      },
      "outputs": [],
      "source": [
        "##TODO train an elastic net on the transformed output of the scaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "en = ElasticNet(alpha=0.01)\n",
        "\n",
        "##TODO train the ElasticNet\n",
        "\n",
        "##TODO predict the testset\n",
        "\n",
        "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, balanced_accuracy_score\n",
        "##TODO print mean squared error and r2 score on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872d1ef8",
      "metadata": {
        "id": "872d1ef8"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27e2756e",
      "metadata": {
        "id": "27e2756e"
      },
      "source": [
        "Next, we train an OLS model doing binary prediction on these movie reviews. Two get two bins, we transform the continuous ratings into two classes, where one class contains all the negative ratings (value < 0.5), the other class all the positive ratings (value > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cbd752c",
      "metadata": {
        "id": "9cbd752c"
      },
      "outputs": [],
      "source": [
        "y_train = [1 if i >= 0.5 else 0 for i in y_train]\n",
        "y_test = [1 if i >= 0.5 else 0 for i in y_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2c239d",
      "metadata": {
        "id": "2c2c239d"
      },
      "outputs": [],
      "source": [
        "##TODO train logistic regression on X_train\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression()\n",
        "\n",
        "##TODO train a logistic regression\n",
        "\n",
        "##TODO predict the testset \n",
        "\n",
        "##since we have continuous output, we need to post-process our labels into two classes. We choose a threshold of 0.5 \n",
        "def map_predictions(predicted):\n",
        "    predicted = [1 if i > 0.5 else 0 for i in predicted]\n",
        "    return predicted\n",
        "\n",
        "##TODO print the accuracy of our classifier on the testset\n",
        "\n",
        "## TODO print the 10 most informative words of the regression (the 10 words having the highest coefficients)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "3hNKx6fUGgCL"
      },
      "id": "3hNKx6fUGgCL"
    },
    {
      "cell_type": "markdown",
      "id": "d0a6bc62",
      "metadata": {
        "id": "d0a6bc62"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the AG news dataset (same as hw01)\n",
        "#Download them from here \n",
        "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "df.columns = [\"label\", \"title\", \"lead\"]\n",
        "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
        "def replace_label(x):\n",
        "\treturn label_map[x]\n",
        "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
        "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
        "df = df.sample(n=10000) # # only use 10K datapoints\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ueplJsWuS_zl"
      },
      "id": "ueplJsWuS_zl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new variable \"business\" that takes value 1 if the label is business and 0 otherwise\n",
        "df['business'] = df['label'].apply(lambda x: int(x=='business'))\n",
        "y = df['business'].values\n",
        "df['business'].head()"
      ],
      "metadata": {
        "id": "df6ZVZfDTBwH"
      },
      "id": "df6ZVZfDTBwH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# pre-process text as you did in HW02\n",
        "def tokenize(x):\n",
        "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit]\n",
        "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenize(x))\n",
        "df[\"preprocessed\"] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "df[\"preprocessed_text\"] = df[\"preprocessed\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "##TODO vectorize the pre-processed text using CountVectorizer"
      ],
      "metadata": {
        "id": "7CFbsYDMTCTt"
      },
      "id": "7CFbsYDMTCTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9b6e66fc",
      "metadata": {
        "id": "9b6e66fc"
      },
      "source": [
        "Your goal here is to use features from the Vectorized text to predict whether the snippet is from a business article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b718ae5",
      "metadata": {
        "id": "0b718ae5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "## TODO build a MLP model with at least 2 hidden layers with ReLU activation, followed by dropout and an output layer with sigmoid activation\n",
        "## TODO summarize the model using torchsummary\n",
        "## TODO fit the model using early stopping to predict the business label\n",
        "# (hint: early stopping means if the validation score does not increase for more than \"patience\" times, training should stop and load the best model so far)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1263d8c1",
      "metadata": {
        "id": "1263d8c1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}