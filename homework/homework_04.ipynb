{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21e1ead",
   "metadata": {
    "id": "d21e1ead"
   },
   "source": [
    "# HW04: ML and DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d1f0b",
   "metadata": {
    "id": "680d1f0b"
   },
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf38c8",
   "metadata": {
    "id": "c9bf38c8"
   },
   "source": [
    "## Load and Pre-process Text\n",
    "We do sentiment analysis on the [Movie Review Data](https://www.cs.cornell.edu/people/pabo/movie-review-data/). If you would like to know more about the data, have a look at [the paper](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.pdf) (but no need to do so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21439804",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21439804",
    "outputId": "a30baf4d-a3c9-4ccb-b667-7991ae9a5c84",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-24 08:59:37--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4029756 (3.8M) [application/x-gzip]\n",
      "Saving to: ‘scale_data.tar.gz.1’\n",
      "\n",
      "scale_data.tar.gz.1 100%[===================>]   3.84M  1.40MB/s    in 2.8s    \n",
      "\n",
      "2023-03-24 08:59:41 (1.40 MB/s) - ‘scale_data.tar.gz.1’ saved [4029756/4029756]\n",
      "\n",
      "--2023-03-24 08:59:41--  https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8853204 (8.4M) [application/x-gzip]\n",
      "Saving to: ‘scale_whole_review.tar.gz.1’\n",
      "\n",
      "scale_whole_review. 100%[===================>]   8.44M  1.90MB/s    in 5.6s    \n",
      "\n",
      "2023-03-24 08:59:47 (1.50 MB/s) - ‘scale_whole_review.tar.gz.1’ saved [8853204/8853204]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this tutorial, we do sentiment analysis\n",
    "# download the data\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar xf aclImdb_v1.tar.gz\n",
    "\n",
    "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
    "!wget https://www.cs.cornell.edu/people/pabo/movie-review-data/scale_whole_review.tar.gz\n",
    " \n",
    "!tar xf scale_data.tar.gz \n",
    "!tar xf scale_whole_review.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685ef2e",
   "metadata": {
    "id": "d685ef2e"
   },
   "source": [
    "First, we have to load the data for which we provide the function below. Note how we also preprocess the text using gensim's simple_preprocess() function and how we already split the data into a train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18a238d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a18a238d",
    "outputId": "39b85e2c-668f-47d4-f332-6edfe679a0a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: for what it worth correctly guessed the identity of the killer in scream well sort of suppose should feel satisfied at my own cleverness since dimension and the makers of scream have put so much effort into keeping that piece of information secret even more so than in the original scream writer kevin williamson goes to ridiculous extremes to keep the audience guessing whodunnit so ridiculous that the film becomes too focused on the one thing which should have been least important as horror film it solid piece of work as satire it frequently hilarious as mystery it tries way way too hard scream takes place two years after the events of the original just in time for hollywood to cash in on the woodsboro high murders the non fiction book by reporter gale weathers courteney cox has become popular horror film called stab which in turn appears to have generated copycat killer when two college students turn up dead at the film premiere sidney prescott neve campbell once again begins to fear for her life other woodsboro survivors like dewey riley david arquette and video store clerk turned film student randy meeks jamie kennedy offer support but bodies continue to turn up at an alarming rate who could the killer be sidney boyfriend derek jerry connell cotton weary liev schreiber the man sidney once wrongly accused of murder gale cameraman joel duane martin film student mickey timothy olyphant reporter debbie salt laurie metcalf gale dewey randy jermaine marlon tito if you can keep the suspects straight without press kit more power to you williamson often seems so concerned with turning every live body in sight into potential suspect that he lets that question overwhelm everything else when the characters aren busy trying to survive the night they re either sitting around trying to figure out who the killer is or around trying to figure out who the killer is it all builds to revelatory climax drawn out so long past the point of diminishing returns loaded with confessions motives and false finishes that you can even see the point of diminishing returns from there being bit harder on scream than it might deserve because that one big problem may be the only real problem williamson satirical darts are expertly targeted not just at horror film cliches or the inevitable inferiority of sequels but at the original film itself the film within film stab provides few wonderful moments as do the audience reactions to some of the more improbable situations hang up the phone and star his the dialogue is sharp smart and not at all afraid of naming names cox friends co stars david schwimmer and jennifer aniston are among those skewered perfect counter point to wes craven crafty suspense scenes if you re not ready to crawl out of your skin when campbell crawls across the unconscious form of her tormentor it time for pulse check scream is so entertaining both as chiller and as chuckler that the lack of attention to those elements is frustration kevin williamson is carving out new genre for himself which certainly seems to be finding an audience both screams and know what you did last summer take off with the radical notion that umasking the monster is good idea there just something deflating about tense comic horror film that turns into scooby doo episode williamson has too much talent as writer of comedy to keep getting side tracked by conventions even if they re conventions he helping to re create scare us or make us laugh we know you can do both just keep the mystery machine parked at the curb on the renshaw scale of visit scott renshaw moviepage http www inconnect com renshaw subscribe to receive new reviews directly by email see the moviepage for details or reply to this message with subject line subscribe \n",
      "label: 0.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    examples, labels = [], []\n",
    "    authors = os.listdir(\"scale_whole_review\")\n",
    "    \n",
    "    for author in authors:\n",
    "        path = os.listdir(os.path.join(\"scale_whole_review\", author, \"txt.parag\"))\n",
    "        fn_ids = os.path.join(\"scaledata\", author, \"id.\" + author)\n",
    "        fn_ratings = os.path.join(\"scaledata\", author, \"rating.\" + author)\n",
    "        \n",
    "        with open(fn_ids) as ids, open(fn_ratings) as ratings:\n",
    "            for idx, rating in zip(ids, ratings):\n",
    "                labels.append(float(rating.strip()))\n",
    "                filename_text = os.path.join(\"scale_whole_review\", author, \"txt.parag\", idx.strip() + \".txt\")\n",
    "                \n",
    "                with open(filename_text, encoding='latin-1') as f:\n",
    "                    examples.append(\" \".join(simple_preprocess(f.read())))\n",
    "                    \n",
    "    return examples, labels\n",
    "                  \n",
    "X,y  = load_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print (\"text:\", X_train[0], \"\\nlabel:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284033cf",
   "metadata": {
    "id": "284033cf"
   },
   "source": [
    "## Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09aff185",
   "metadata": {
    "id": "09aff185",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train a TF_IDF Vectorizer on X_train and vectorize X_train and X_test\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                        max_df=.5,  \n",
    "                        stop_words='english',\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "##TODO train vectorizer\n",
    "TfidfVectorizer_fitted = vec.fit(X_train)\n",
    "\n",
    "##TODO transform X_train to TF-IDF values\n",
    "X_train_tfidf = TfidfVectorizer_fitted.transform(X_train)\n",
    "\n",
    "##TODO transform X_test to TF-IDF values\n",
    "X_test_tfidf = TfidfVectorizer_fitted.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d44dbb",
   "metadata": {
    "id": "58d44dbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##TODO scale both training and test data with the standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "# Note: We need to use the same scaler for test and train set!\n",
    "X_train_scale = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_scale = scaler.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d8a57",
   "metadata": {
    "id": "ad9d8a57"
   },
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5f4520",
   "metadata": {
    "id": "1e5f4520",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mse= 0.016879907696919678\n",
      "test_r2= 0.4848673914767738\n"
     ]
    }
   ],
   "source": [
    "##TODO train an elastic net on the transformed output of the scaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet(alpha=0.01)\n",
    "\n",
    "##TODO train the ElasticNet\n",
    "en.fit(X_train_scale, y_train)\n",
    "\n",
    "##TODO predict the testset\n",
    "y_test_pred = en.predict(X_test_scale)\n",
    "\n",
    "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, balanced_accuracy_score\n",
    "##TODO print mean squared error and r2 score on the test set\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"test_mse=\", test_mse)\n",
    "print(\"test_r2=\", test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d1ef8",
   "metadata": {
    "id": "872d1ef8"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e2756e",
   "metadata": {
    "id": "27e2756e"
   },
   "source": [
    "Next, we train an OLS model doing binary prediction on these movie reviews. Two get two bins, we transform the continuous ratings into two classes, where one class contains all the negative ratings (value < 0.5), the other class all the positive ratings (value > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbd752c",
   "metadata": {
    "id": "9cbd752c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = [1 if i >= 0.5 else 0 for i in y_train]\n",
    "y_test = [1 if i >= 0.5 else 0 for i in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c2c239d",
   "metadata": {
    "id": "2c2c239d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8002421307506054\n",
      "\n",
      "10 Most important words:\n",
      "unfunny\n",
      "bad\n",
      "boring\n",
      "supposed\n",
      "lifeless\n",
      "inane\n",
      "ludicrous\n",
      "signature\n",
      "fails\n",
      "ridiculous\n"
     ]
    }
   ],
   "source": [
    "##TODO train logistic regression on X_train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression(max_iter=500)\n",
    "\n",
    "##TODO train a logistic regression\n",
    "logistic_regression.fit(X_train_scale, y_train)\n",
    "\n",
    "##TODO predict the testset \n",
    "y_test_pred = logistic_regression.predict(X_test_scale)\n",
    "\n",
    "##since we have continuous output, we need to post-process our labels into two classes. We choose a threshold of 0.5 \n",
    "def map_predictions(predicted):\n",
    "    predicted = [1 if i > 0.5 else 0 for i in predicted]\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "y_test_pred_mapped = map_predictions(y_test_pred)\n",
    "\n",
    "##TODO print the accuracy of our classifier on the testset\n",
    "accuracy = accuracy_score(y_test, y_test_pred_mapped)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "## TODO print the 10 most informative words of the regression (the 10 words having the highest coefficients)\n",
    "\n",
    "# Note: We use the TD IDF vectoriezrs above and correlate it with the\n",
    "# logistic regression\n",
    "coeffs = logistic_regression.coef_[0]\n",
    "\n",
    "\n",
    "# Get words\n",
    "words = vec.get_feature_names_out()\n",
    "\n",
    "# Create (coeff, word) pairs\n",
    "pairs = list(zip(coeffs, words))\n",
    "\n",
    "# By default, sort acts on the first element of the tuples - it's inplace\n",
    "pairs.sort()\n",
    "\n",
    "print(\"\\n10 Most important words:\")\n",
    "for i in range(10):\n",
    "    print(pairs[i][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3hNKx6fUGgCL",
   "metadata": {
    "id": "3hNKx6fUGgCL"
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6bc62",
   "metadata": {
    "id": "d0a6bc62"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ueplJsWuS_zl",
   "metadata": {
    "id": "ueplJsWuS_zl",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38427</th>\n",
       "      <td>world</td>\n",
       "      <td>Durable Goods, Lower Oil Lift Stocks</td>\n",
       "      <td>NEW YORK - Stocks edged higher Friday as a dip...</td>\n",
       "      <td>Durable Goods, Lower Oil Lift Stocks NEW YORK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69898</th>\n",
       "      <td>sport</td>\n",
       "      <td>Mississippi St. Upsets No. 20 Florida (AP)</td>\n",
       "      <td>AP - Sylvester Croom's first big win for Missi...</td>\n",
       "      <td>Mississippi St. Upsets No. 20 Florida (AP) AP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80590</th>\n",
       "      <td>business</td>\n",
       "      <td>Verizon Wireless, Nextel Reach Accord</td>\n",
       "      <td>Verizon Wireless has agreed to drop its opposi...</td>\n",
       "      <td>Verizon Wireless, Nextel Reach Accord Verizon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107676</th>\n",
       "      <td>sport</td>\n",
       "      <td>Time is of the essence in this latest round</td>\n",
       "      <td>Next Thursday and Friday NHL executives, inclu...</td>\n",
       "      <td>Time is of the essence in this latest round Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91870</th>\n",
       "      <td>sport</td>\n",
       "      <td>Mike Selvey</td>\n",
       "      <td>There is a sense of symmetry about Michael Vau...</td>\n",
       "      <td>Mike Selvey There is a sense of symmetry about...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                        title  \\\n",
       "38427      world         Durable Goods, Lower Oil Lift Stocks   \n",
       "69898      sport   Mississippi St. Upsets No. 20 Florida (AP)   \n",
       "80590   business        Verizon Wireless, Nextel Reach Accord   \n",
       "107676     sport  Time is of the essence in this latest round   \n",
       "91870      sport                                  Mike Selvey   \n",
       "\n",
       "                                                     lead  \\\n",
       "38427   NEW YORK - Stocks edged higher Friday as a dip...   \n",
       "69898   AP - Sylvester Croom's first big win for Missi...   \n",
       "80590   Verizon Wireless has agreed to drop its opposi...   \n",
       "107676  Next Thursday and Friday NHL executives, inclu...   \n",
       "91870   There is a sense of symmetry about Michael Vau...   \n",
       "\n",
       "                                                     text  \n",
       "38427   Durable Goods, Lower Oil Lift Stocks NEW YORK ...  \n",
       "69898   Mississippi St. Upsets No. 20 Florida (AP) AP ...  \n",
       "80590   Verizon Wireless, Nextel Reach Accord Verizon ...  \n",
       "107676  Time is of the essence in this latest round Ne...  \n",
       "91870   Mike Selvey There is a sense of symmetry about...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df6ZVZfDTBwH",
   "metadata": {
    "id": "df6ZVZfDTBwH",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38427     0\n",
       "69898     0\n",
       "80590     1\n",
       "107676    0\n",
       "91870     0\n",
       "Name: business, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new variable \"business\" that takes value 1 if the label is business and 0 otherwise\n",
    "df['business'] = df['label'].apply(lambda x: int(x=='business'))\n",
    "y = df['business'].values\n",
    "df['business'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7CFbsYDMTCTt",
   "metadata": {
    "id": "7CFbsYDMTCTt",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(x):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [w\u001b[38;5;241m.\u001b[39mlemma_\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m nlp(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_stop \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_punct \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_digit]\n\u001b[0;32m----> 8\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n",
      "File \u001b[0;32m~/ETH/12_semester/nlp_lss_2023/env/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ETH/12_semester/nlp_lss_2023/env/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ETH/12_semester/nlp_lss_2023/env/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/ETH/12_semester/nlp_lss_2023/env/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(x):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [w\u001b[38;5;241m.\u001b[39mlemma_\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m nlp(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_stop \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_punct \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_digit]\n\u001b[0;32m----> 8\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(x):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [w\u001b[38;5;241m.\u001b[39mlemma_\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_stop \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_punct \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mis_digit]\n",
      "File \u001b[0;32m~/ETH/12_semester/nlp_lss_2023/env/lib/python3.10/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# pre-process text as you did in HW02\n",
    "def tokenize(x):\n",
    "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit]\n",
    "df[\"tokens\"] = df[\"text\"].apply(lambda x: tokenize(x))\n",
    "df[\"preprocessed\"] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "df[\"preprocessed_text\"] = df[\"preprocessed\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "##TODO vectorize the pre-processed text using CountVectorizer\n",
    "countVectorizer = CountVectorizer(min_df=0.01, # at min 1% of docs\n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        max_df=.9,  \n",
    "                        ngram_range=(1,3))\n",
    "\n",
    "X = countVectorizer.fit_transform(df['preprocessed'])\n",
    "vocab = countVectorizer.get_feature_names_out()\n",
    "\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a69f8f-c63c-482a-8110-858b7066243d",
   "metadata": {},
   "source": [
    "NOTE: For some reason, the above never finished running on my notebook and I run out of time to dig into it. This also means, I couldn't write the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e66fc",
   "metadata": {
    "id": "9b6e66fc"
   },
   "source": [
    "Your goal here is to use features from the Vectorized text to predict whether the snippet is from a business article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b718ae5",
   "metadata": {
    "id": "0b718ae5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## TODO build a MLP model with at least 2 hidden layers with ReLU activation, followed by dropout and an output layer with sigmoid activation\n",
    "## TODO summarize the model using torchsummary\n",
    "## TODO fit the model using early stopping to predict the business label\n",
    "# (hint: early stopping means if the validation score does not increase for more than \"patience\" times, training should stop and load the best model so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263d8c1",
   "metadata": {
    "id": "1263d8c1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
