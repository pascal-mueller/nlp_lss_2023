{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe9d25b",
   "metadata": {
    "id": "dbe9d25b"
   },
   "source": [
    "# HW05: Word Embeddings\n",
    "\n",
    "Remember that these homework work as a completion grade. **You can <span style=\"color:red\">not</span> skip one section this homework.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a0596",
   "metadata": {
    "id": "8b3a0596"
   },
   "source": [
    "**Essay Feedback**\n",
    "\n",
    "Please provide feedback to two classmates' essays on Eduflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea14794",
   "metadata": {
    "id": "5ea14794"
   },
   "source": [
    "**Training word2vec**\n",
    "\n",
    "In this section, we train a word2vec model using gensim. We train the model on text8 (which consists of the first 90M characters of a Wikipedia dump from 2006 and is considered one of the benchmarks for evaluating language models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a38d6e",
   "metadata": {
    "id": "95a38d6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_records': 1701,\n",
       " 'record_format': 'list of str (tokens)',\n",
       " 'file_size': 33182058,\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'license': 'not found',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'file_name': 'text8.gz',\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'parts': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "api.info(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a49444c",
   "metadata": {
    "id": "0a49444c"
   },
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fa38b5",
   "metadata": {
    "id": "61fa38b5"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "##TODO train a word2vec model on this dataset which appear at least 10 times in the corpus\n",
    "\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
    "\n",
    "# Train model\n",
    "model = Word2Vec(dataset, min_count =  10)\n",
    "\n",
    "# Get word vectors\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Store word vectors to disk\n",
    "word_vectors.save('vectors.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af69360",
   "metadata": {
    "id": "4af69360"
   },
   "source": [
    "**Word Similarities**\n",
    "\n",
    "gensim models provide almost all the utility you might want to wish for to perform standard word similarity tasks. They are available in the .wv (wordvectors) attribute of the model, more details could be found [here](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf99280",
   "metadata": {
    "id": "5cf99280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=100, 47134 keys>\n",
      "[('prince', 0.741571843624115), ('emperor', 0.7393261790275574), ('queen', 0.7222601175308228), ('throne', 0.7127467393875122), ('regent', 0.683900773525238), ('vii', 0.6831018924713135), ('aragon', 0.679315447807312), ('sultan', 0.6753013730049133), ('kings', 0.6689374446868896), ('viii', 0.6606752276420593)]\n"
     ]
    }
   ],
   "source": [
    "##TODO find the closest words to king\n",
    "\n",
    "# Ref 1: https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load word vectors form disk\n",
    "word_vectors = KeyedVectors.load('vectors.kv')\n",
    "\n",
    "print(word_vectors)\n",
    "\n",
    "# Result is a list of tuples: (most_similar_key, cosine similarity)]\n",
    "result = word_vectors.most_similar(positive=['king'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30c847",
   "metadata": {
    "id": "9c30c847"
   },
   "source": [
    "King is to man as woman is to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615b6116",
   "metadata": {
    "id": "615b6116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6665955185890198), ('throne', 0.6042353510856628), ('prince', 0.6033719778060913), ('empress', 0.6009417176246643), ('princess', 0.5853360295295715), ('isabella', 0.5832987427711487), ('son', 0.5782522559165955), ('emperor', 0.5712879300117493), ('aragon', 0.5683165192604065), ('elizabeth', 0.5633431673049927)]\n"
     ]
    }
   ],
   "source": [
    "##TODO find the closest word for the vector \"woman\" + \"king\" - \"man\"\n",
    "\n",
    "# Ref 1: https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar\n",
    "\n",
    "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af37627",
   "metadata": {
    "id": "2af37627"
   },
   "source": [
    "**Evaluate Word Similarities** \n",
    "\n",
    "One common way to evaluate word2vec models are word analogy tasks. Let's check how good our model is on one of those. We consider the [WordSim353](http://alfonseca.org/eng/research/wordsim353.html) benchmark, the task is to determine how similar two words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71515b20",
   "metadata": {
    "id": "71515b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-31 14:32:23--  http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
      "Resolving alfonseca.org (alfonseca.org)... 162.215.249.67\n",
      "Connecting to alfonseca.org (alfonseca.org)|162.215.249.67|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5460 (5.3K) [application/x-gzip]\n",
      "Saving to: ‘ws353simrel.tar.gz.8’\n",
      "\n",
      "ws353simrel.tar.gz. 100%[===================>]   5.33K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-03-31 14:32:24 (179 MB/s) - ‘ws353simrel.tar.gz.8’ saved [5460/5460]\n",
      "\n",
      "[('tiger', 'cat'), ('tiger', 'tiger'), ('plane', 'car')] [7.35, 10.0, 5.77]\n"
     ]
    }
   ],
   "source": [
    "!wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
    "!tar xf ws353simrel.tar.gz\n",
    "\n",
    "path = \"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "\n",
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            X.append((line[0], line[1])) # each entry in x contains two words, e.g. X[0] = (tiger, cat)\n",
    "            y.append(float(line[-1])) # each entry in y is the annotation how similar two words are, e.g. Y[0] = 7.35\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(path)\n",
    "print (X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8ced33",
   "metadata": {
    "id": "9c8ced33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PearsonRResult(statistic=0.6820281339003748, pvalue=3.979966014111241e-29), SignificanceResult(statistic=0.666789919894723, pvalue=1.8004414358901796e-27), 0.9852216748768473)\n"
     ]
    }
   ],
   "source": [
    "##TODO compute how similar the pairs in the WordSim353 are according to our model\n",
    "# if a word is not present in our model, we assign similarity 0 for the respective text pair\n",
    "import io\n",
    "\n",
    "# Ref: https://radimrehurek.com/gensim/models/keyedvectors.html#what-can-i-do-with-word-vectors\n",
    "\n",
    "fn = lambda pair, value : f\"{pair[0]},{pair[1]},{value}\"\n",
    "\n",
    "dataset_map = map(fn,X, y)\n",
    "\n",
    "data = list(dataset_map)\n",
    "\n",
    "# Store the data as a csv so we can pass it to the function below\n",
    "# The function below only takes data from file, not form memory......\n",
    "data_string = \"\\n\".join(data)\n",
    "f = open('data.csv','w')\n",
    "f.write(data_string)\n",
    "f.close()\n",
    "\n",
    "# \"Apply\" our model to the gold standard aka human annnotations\n",
    "similarities = word_vectors.evaluate_word_pairs('data.csv', delimiter=\",\", dummy4unknown=True)\n",
    "\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebd47f93",
   "metadata": {
    "id": "ebd47f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PearsonRResult(statistic=0.6869470526204524, pvalue=1.1061160166189108e-29), SignificanceResult(statistic=0.6695084153849175, pvalue=9.27314443825835e-28), 0.9852216748768473)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "##TODO compute spearman's rank correlation between our prediction and the human annotations\n",
    "\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec86899",
   "metadata": {
    "id": "9ec86899"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "##TODO compute word similarities in the WordSim353 dataset using spaCy word embeddings\n",
    "##TODO compute spearman's rank correlation between these similarities and the human annotations\n",
    "# Don't worry if results are not too convincing for this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29de774",
   "metadata": {
    "id": "d29de774"
   },
   "source": [
    "**PyTorch Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927e048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:42:21.281177Z",
     "start_time": "2022-03-22T21:42:21.208787Z"
    },
    "id": "3927e048"
   },
   "outputs": [],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "# !wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d6b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:20.385383Z",
     "start_time": "2022-03-22T21:40:18.447956Z"
    },
    "id": "a49d6b6e"
   },
   "outputs": [],
   "source": [
    "vocab = 200\n",
    "##TODO tokenize the text, only keep 200 most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0f840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:23.322875Z",
     "start_time": "2022-03-22T21:40:23.311923Z"
    },
    "id": "c4c0f840"
   },
   "outputs": [],
   "source": [
    "length = 100\n",
    "#TODO create a one_hot representation for each word and truncate/pad the sequences such that they are all of the same length (here we use 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d193dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-22T21:40:28.364553Z",
     "start_time": "2022-03-22T21:40:28.354695Z"
    },
    "id": "c3d193dd"
   },
   "outputs": [],
   "source": [
    "\n",
    "##TODO create your torch embedding like we did in notebook 5! (hint: predicting labels: world, sport, business, and sci/tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac3bd2",
   "metadata": {
    "id": "2cac3bd2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
