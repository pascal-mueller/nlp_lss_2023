{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60bffde",
   "metadata": {},
   "source": [
    "# HW02: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed3f1a",
   "metadata": {},
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc19cf83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters)</td>\n",
       "      <td>Reuters - Stocks ended slightly higher on Frid...</td>\n",
       "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              title  \\\n",
       "0  business  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "1  business    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "2  business  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "3  business  Oil prices soar to all-time record, posing new...   \n",
       "4  business        Stocks End Up, But Near Year Lows (Reuters)   \n",
       "\n",
       "                                                lead  \\\n",
       "0  Reuters - Private investment firm Carlyle Grou...   \n",
       "1  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "2  Reuters - Authorities have halted oil export\\f...   \n",
       "3  AFP - Tearaway world oil prices, toppling reco...   \n",
       "4  Reuters - Stocks ended slightly higher on Frid...   \n",
       "\n",
       "                                                text  \n",
       "0  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
       "1  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
       "2  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
       "3  Oil prices soar to all-time record, posing new...  \n",
       "4  Stocks End Up, But Near Year Lows (Reuters) Re...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7b626",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1861a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux under threat from  #39;security update #39; Linux maker Red Hat is warning users about an email that pretends to be an official security advisory but is actually a phishing-type scam that contains links to malicious code.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "dfs = df.sample(50)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "##TODO use spacy to split the documents in the sampled dataframe (dfs) in sentences and tokens\n",
    "dfs['text_tokenized'] = dfs['text'].apply(nlp)\n",
    "\n",
    "##TODO print the first sentence of the first document in your sample\n",
    "print(list(dfs.iloc[0][\"text_tokenized\"].sents)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65fa4c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux npadvmod\n",
      "under prep\n",
      "threat pobj\n",
      "from prep\n",
      "  dep\n",
      "# nmod\n",
      "39;security pobj\n",
      "update ccomp\n",
      "# nmod\n",
      "39 dobj\n",
      "; punct\n",
      "Linux compound\n",
      "maker compound\n",
      "Red compound\n",
      "Hat nsubj\n",
      "be aux\n",
      "warn ROOT\n",
      "user dobj\n",
      "about prep\n",
      "an det\n",
      "email pobj\n",
      "that nsubj\n",
      "pretend relcl\n",
      "to aux\n",
      "be xcomp\n",
      "an det\n",
      "official amod\n",
      "security compound\n",
      "advisory attr\n",
      "but cc\n",
      "be conj\n",
      "actually advmod\n",
      "a det\n",
      "phishe amod\n",
      "- punct\n",
      "type compound\n",
      "scam attr\n",
      "that nsubj\n",
      "contain relcl\n",
      "link dobj\n",
      "to prep\n",
      "malicious amod\n",
      "code pobj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "## TODO create a new column with tokens in lowercase (x.lower()),\n",
    "## without punctuation tokens (x.is_punct), stopwords (x.is_stop), and digits (x.is_digit)\n",
    "\n",
    "def clean_tokens(sentence):\n",
    "    for word in sentence:\n",
    "        if not word.is_stop and not word.is_punct and not word.is_digit:\n",
    "            return word.lemma_.lower() \n",
    "\n",
    "dfs['text_tokenized_cleaned'] = dfs['text_tokenized'].apply(clean_tokens)\n",
    "\n",
    "\n",
    "## TODO print the tokens (x.lemma_) and the dependency labels (x.dep_ )\n",
    "## of the first sentence of the first document (doc.sents)\n",
    "first_sentence = list(dfs.iloc[0][\"text_tokenized\"].sents)[0]\n",
    "for word in first_sentence:\n",
    "  print(word.lemma_, word.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef544f1",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Let's compute the ratio of named entities starting with a capital letter, e.g. if we have \"University of Chicago\" as a NE, \"University\" and \"Chicago\" are capitalized, \"of\" is not, thus the ratio is 2/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be2cb111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.116\n"
     ]
    }
   ],
   "source": [
    "## TODO print the ratio of tokens being part of a named entity span\n",
    "## starting with a capital letter (doc.ents)\n",
    "\n",
    "# Vry unclear what to do. Ratio respective to what? The whole dataset? A sentence? What?\n",
    "\n",
    "num_capitalized = 0\n",
    "num_words = 0\n",
    "\n",
    "# Loop throuhg all tokens. A token is a sentence.\n",
    "for sentence in dfs[\"text_tokenized\"]:\n",
    "    for named_entity in sentence.ents:\n",
    "        for word in named_entity:\n",
    "            if word.is_upper:\n",
    "                num_capitalized += 1\n",
    "            \n",
    "            # Increate total word counter\n",
    "            num_words += 1\n",
    "\n",
    "# Compute ratio\n",
    "print(num_capitalized / num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984b4d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013134851138353765\n"
     ]
    }
   ],
   "source": [
    "##TODO print the ratio of capitalized tokens not being part of a named entity span (have no token.ent_type_)\n",
    "# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized\n",
    "\n",
    "# NOTE: It is really really hard to fully grasp what you want me\n",
    "# to compute. Please state the TODO clearer. \"The dog barks\" is no named entity,\n",
    "# yet we have to compute some ratio of named identities and upper case letters.\n",
    "# Seems to me like the example is wrong or the description just doesn't make sense to me.\n",
    "\n",
    "num_capitalized = 0\n",
    "num_words= 0\n",
    "\n",
    "for sentence in dfs[\"text_tokenized\"]:\n",
    "    for word in sentence:\n",
    "        # Words that are upper case but aren't a named entity\n",
    "        if word.is_upper and not word.ent_type_:\n",
    "            num_capitalized += 1\n",
    "        \n",
    "        num_words += 1\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "# Compute ratio\n",
    "print(num_capitalized / num_words)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3ede3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012259194395796848\n"
     ]
    }
   ],
   "source": [
    "## TODO print the ratio of capitalized tokens not being\n",
    "## a named entity and not being the first token in a sentence\n",
    "# e.g. \"The dog barks\" = 0; 3 tokens, \"The\" is capitalized but the starting token of a sentence, no other tokens are capitalized.\n",
    "\n",
    "# Note: again, very unclear what is asked.\n",
    "\n",
    "num_capitalized = 0\n",
    "num_words = 0\n",
    "\n",
    "for document in dfs[\"text_tokenized\"]:\n",
    "    for i, word in enumerate(document):\n",
    "        if not word.ent_type_ and i > 0:\n",
    "            if word.is_upper:\n",
    "                num_capitalized += 1\n",
    "        \n",
    "        num_words += 1\n",
    "        \n",
    "\n",
    "print(num_capitalized / num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466753be",
   "metadata": {},
   "source": [
    "Give an example of a capitalized token in the data which is neither a named entity nor at the start of a sentence. What could be the reason the token is capitalized (one sentence)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e21c0a01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SD ]  Industry #39;s First  #39;Smart #39; SD Memory Card A Secure Digital (SD) card is a tiny memory card used to make storage portable among various devices, such as car navigation systems, cellular phones, eBooks, PDAs, smartphones, digital cameras, music players, camcorders, and personal computers.\n"
     ]
    }
   ],
   "source": [
    "break_flag = False\n",
    "\n",
    "for document in dfs[\"text_tokenized\"]:\n",
    "    if break_flag:\n",
    "        break\n",
    "        \n",
    "    for i, word in enumerate(document):\n",
    "        if i > 0 and word.is_upper and not word.ent_type_:\n",
    "            print(\"[\", word, \"] \", document)\n",
    "            break_flag = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea1802-1886-4d28-8705-4bf934215728",
   "metadata": {},
   "source": [
    "The above should output SD as a word that isn't a named entity but also capitalized. It stands for Secure Digital and is an abbreviation of it. So abbreviations are an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3126a5c7",
   "metadata": {},
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055edce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##TODO using the whole sample, produce a world cloud with bigrams for label == business using tfidf frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948e50f",
   "metadata": {},
   "source": [
    "## Supervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "##TODO compute the number of words per document (excluding stopwords)\n",
    "##TODO get the most predictive features of the number of words per document using first f_class and then chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaee2e4",
   "metadata": {},
   "source": [
    "Are the results different? What could be a reason for this? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af74a1d",
   "metadata": {},
   "source": [
    "## Huggingface Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e75e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we use distilbert tokenizer\n",
    "# !pip install transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# let's instantiate a tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "##TODO tokenize the sentences in the sampled dataframe (dfs) using the DisilBertTokenizer\n",
    "##TODO what is the type/token ratio from this tokenizer (number_of_unqiue_token_types/number_of_tokens)?\n",
    "##TODO what is the amount of subword tokens returned by the huggingface tokenizer? hint: each subword token starts with \"#\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc9d00",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16bf8639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110581</th>\n",
       "      <td>sport</td>\n",
       "      <td>UEFA to probe Valencia-Werder Bremen incidents</td>\n",
       "      <td>UEFA will be launching disciplinary proceeding...</td>\n",
       "      <td>UEFA to probe Valencia-Werder Bremen incidents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15925</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>New iMac tries to play it cool</td>\n",
       "      <td>Hot G5 chip requires some serious effort to av...</td>\n",
       "      <td>New iMac tries to play it cool Hot G5 chip req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46720</th>\n",
       "      <td>business</td>\n",
       "      <td>Ford Reports Disappointing U.S. Sales</td>\n",
       "      <td>Ford's car and truck business sales fell nearl...</td>\n",
       "      <td>Ford Reports Disappointing U.S. Sales Ford's c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57350</th>\n",
       "      <td>sci/tech</td>\n",
       "      <td>Microsoft Upgrades Windows XP Media Center (Ne...</td>\n",
       "      <td>NewsFactor - Bill Gates is about to announce a...</td>\n",
       "      <td>Microsoft Upgrades Windows XP Media Center (Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90601</th>\n",
       "      <td>sport</td>\n",
       "      <td>Rams make statement they #39;re team to beat i...</td>\n",
       "      <td>ST. LOUIS -- When St. Louis coach Mike Martz l...</td>\n",
       "      <td>Rams make statement they #39;re team to beat i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                              title  \\\n",
       "110581     sport     UEFA to probe Valencia-Werder Bremen incidents   \n",
       "15925   sci/tech                     New iMac tries to play it cool   \n",
       "46720   business              Ford Reports Disappointing U.S. Sales   \n",
       "57350   sci/tech  Microsoft Upgrades Windows XP Media Center (Ne...   \n",
       "90601      sport  Rams make statement they #39;re team to beat i...   \n",
       "\n",
       "                                                     lead  \\\n",
       "110581  UEFA will be launching disciplinary proceeding...   \n",
       "15925   Hot G5 chip requires some serious effort to av...   \n",
       "46720   Ford's car and truck business sales fell nearl...   \n",
       "57350   NewsFactor - Bill Gates is about to announce a...   \n",
       "90601   ST. LOUIS -- When St. Louis coach Mike Martz l...   \n",
       "\n",
       "                                                     text  \n",
       "110581  UEFA to probe Valencia-Werder Bremen incidents...  \n",
       "15925   New iMac tries to play it cool Hot G5 chip req...  \n",
       "46720   Ford Reports Disappointing U.S. Sales Ford's c...  \n",
       "57350   Microsoft Upgrades Windows XP Media Center (Ne...  \n",
       "90601   Rams make statement they #39;re team to beat i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df = df.sample(n=10000) # # only use 10K datapoints\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ec5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#TODO preprocess the corpus using spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ae310",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_verb_pairs(sent):\n",
    "    subjs = [w for w in sent if w.dep_ == \"nsubj\"]\n",
    "    pairs = [(w.lemma_.lower(), w.head.lemma_.lower()) for w in subjs]\n",
    "    return pairs\n",
    "##TODO extract the subject-verbs pairs and print the result for the second document\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c30e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for verbs-object pairs ('dobj')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad55039",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO do the same for adjectives-nouns pairs ('amod')\n",
    "##TODO create a list ranking the most common pairs and print the first 10 items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1273253",
   "metadata": {},
   "source": [
    "### Exploring cross label dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d19500",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO extract all the subject-verbs and verbs-object pairs for the verb \"rise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75649d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO for each label create a list ranking the most common subject-verbs pairs and one for the most common verbs-object pairs\n",
    "##TODO print the 10 most common pairs for each of the two lists for the labels \"world\" and \"sci/tech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d363b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d73fbafa1c1ce4c8cd6f07277ecbf2d62d7720c8bfdb6bdbaeeaf6dba6dd25dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
